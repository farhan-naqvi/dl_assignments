{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vxBibviVAhN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import keras \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Dropout, Embedding\n",
        "from keras.models import Sequential, Model \n",
        "from keras.optimizers import SGD \n",
        "from keras.utils import np_utils\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import keras.backend as K\n",
        "\n",
        "data = \"farhan is here bacha , be carefull of, him.\"\n",
        "\n",
        "words = data.split()\n",
        "words = [x.lower().replace(\",\",\"\").replace(\".\",\"\") for x in words]\n",
        "vocab = set(words)\n",
        "vocab_len = len(vocab)\n",
        "word_to_idx = {j:i for i,j in enumerate(words)}\n",
        "idx_to_word = {i:j for i,j in enumerate(words)} \n",
        "\n",
        "c=2\n",
        "dim=100\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for index, i in enumerate(words):\n",
        "    \n",
        "    #edge case\n",
        "    if index <= 1 or index >= len(words)-2:\n",
        "        continue\n",
        "        \n",
        "    # sliding context window \n",
        "    start = index - c\n",
        "    end = index + c + 1\n",
        "    # 2 words Left and 2 words Right\n",
        "    context = words[start:end]\n",
        "    context.remove(i)\n",
        "    target = i\n",
        "    \n",
        "    # data\n",
        "    X_train.append([word_to_idx[w] for w in context])\n",
        "    Y_train.append([word_to_idx[target]])\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_train = np_utils.to_categorical(Y_train, vocab_len)\n",
        "\n",
        "cbow = Sequential() \n",
        "cbow.add(Embedding(input_dim=vocab_len, output_dim=dim, input_length= c*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
        "cbow.add(Dense(vocab_len, activation='softmax'))\n",
        "\n",
        "cbow.compile(loss=\"categorical_crossentropy\", optimizer=SGD(), metrics=[\"accuracy\"])\n",
        "cbow.fit(X_train, Y_train, epochs=5)\n",
        "\n",
        "pred = cbow.predict(X_train)\n",
        "\n",
        "pred = pred.argmax(axis=1)\n",
        "\n",
        "i = 0\n",
        "for context in X_train:\n",
        "    print(\"...............................\")\n",
        "    print(\"===========Context :===========\")\n",
        "    for w in context:\n",
        "        print(idx_to_word[w])\n",
        "    print(\"=========Predicted Target :====\")\n",
        "    print(idx_to_word[pred[i]])\n",
        "    i+=1"
      ]
    }
  ]
}