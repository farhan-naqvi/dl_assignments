{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_GcJlVrDgBk"
   },
   "source": [
    "Name: **Syed Farhan Naqvi**<br>\n",
    "Div: **BE11-Q11**<br>\n",
    "Roll no: **43344**<br>\n",
    "Title: **Assignment 5: Implement the Continuous Bag of Words (CBOW) Model**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "V51q50EbF-T9"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wBUwYdBJElVz"
   },
   "outputs": [],
   "source": [
    "#taking random sentences as data\n",
    "data = \"\"\"My fellow Americans: Four years ago, we launched a great national effort to rebuild our country, to renew its spirit, and to restore the allegiance of this government to its citizens.  In short, we embarked on a mission to make America great again— for all Americans.\n",
    "\n",
    "As I conclude my term as the 45th President of the United States, I stand before you truly proud of what we have achieved together.  We did what we came here to do—and so much more.\n",
    "\n",
    "This week, we inaugurate a new administration and pray for its success in keeping America safe and prosperous.  We extend our best wishes, and we also want them to have luck—a very important word.\n",
    "\n",
    "I’d like to begin by thanking just a few of the amazing people who made our remarkable journey possible.\n",
    "\n",
    "First, let me express my overwhelming gratitude for the love and support of our spectacular First Lady, Melania.  Let me also share my deepest appreciation to my daughter Ivanka, my son-in-law Jared, and to Barron, Don, Eric, Tiffany, and Lara.  You fill my world with light and with joy.\n",
    "\n",
    "I also want to thank Vice President Mike Pence, his wonderful wife Karen, and the entire Pence family.\n",
    "\n",
    "Thank you as well to my Chief of Staff, Mark Meadows; the dedicated members of the White House Staff and the Cabinet; and all the incredible people across our administration who poured out their heart and soul to fight for America.\n",
    "\n",
    "I also want to take a moment to thank a truly exceptional group of people: the United States Secret Service.  My family and I will forever be in your debt.  My profound gratitude as well to everyone in the White House Military Office, the teams of Marine One and Air Force One, every member of the Armed Forces, and state and local law enforcement all across our country.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRil4zLyFCXa",
    "outputId": "8f933c69-f13d-479a-9a4c-dd73f32f10d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FARHAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word.lower() for word in data.split() if word.lower() not in stop]\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "celNk9LmEvm8",
    "outputId": "2f4af625-6344-4d47-c288-539809ea783b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 179\n",
      "Vocabulary Sample: [('to', 1), ('and', 2), ('the', 3), ('my', 4), ('of', 5), ('we', 6), ('a', 7), ('our', 8), ('in', 9), ('i', 10), ('for', 11), ('as', 12), ('also', 13), ('its', 14), ('america', 15), ('all', 16), ('you', 17), ('want', 18), ('people', 19), ('thank', 20)]\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 \n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AAxNYDanInQC"
   },
   "outputs": [],
   "source": [
    "#generating (context word, target/label word) pairs\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb5dNmoZKZBv",
    "outputId": "323adc4e-2495-4ca8-f458-775c41be100c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 100)            17900     \n",
      "                                                                 \n",
      " lambda_1 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 179)               18079     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,979\n",
      "Trainable params: 35,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(cbow.summary())\n",
    "\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs12C3MDK1q4",
    "outputId": "8fc32ea3-e63c-4f88-e31a-ac037c1847f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 1607.3180232048035\n",
      "\n",
      "Epoch: 2 \tLoss: 1543.9130997657776\n",
      "\n",
      "Epoch: 3 \tLoss: 1529.476705789566\n",
      "\n",
      "Epoch: 4 \tLoss: 1523.1182062625885\n",
      "\n",
      "Epoch: 5 \tLoss: 1519.2799623012543\n",
      "\n",
      "Epoch: 6 \tLoss: 1516.6812961101532\n",
      "\n",
      "Epoch: 7 \tLoss: 1514.2458634376526\n",
      "\n",
      "Epoch: 8 \tLoss: 1511.9314937591553\n",
      "\n",
      "Epoch: 9 \tLoss: 1509.589139699936\n",
      "\n",
      "Epoch: 10 \tLoss: 1507.1182687282562\n",
      "\n",
      "Epoch: 11 \tLoss: 1504.2775642871857\n",
      "\n",
      "Epoch: 12 \tLoss: 1501.0132565498352\n",
      "\n",
      "Epoch: 13 \tLoss: 1497.1641418933868\n",
      "\n",
      "Epoch: 14 \tLoss: 1492.619068145752\n",
      "\n",
      "Epoch: 15 \tLoss: 1487.4806954860687\n",
      "\n",
      "Epoch: 16 \tLoss: 1481.792938709259\n",
      "\n",
      "Epoch: 17 \tLoss: 1475.740296125412\n",
      "\n",
      "Epoch: 18 \tLoss: 1469.582598209381\n",
      "\n",
      "Epoch: 19 \tLoss: 1463.2615571022034\n",
      "\n",
      "Epoch: 20 \tLoss: 1456.9802780151367\n",
      "\n",
      "Epoch: 21 \tLoss: 1450.9542870521545\n",
      "\n",
      "Epoch: 22 \tLoss: 1445.262200832367\n",
      "\n",
      "Epoch: 23 \tLoss: 1439.842805504799\n",
      "\n",
      "Epoch: 24 \tLoss: 1434.4847514629364\n",
      "\n",
      "Epoch: 25 \tLoss: 1429.7269642353058\n",
      "\n",
      "Epoch: 26 \tLoss: 1425.7536908388138\n",
      "\n",
      "Epoch: 27 \tLoss: 1422.5108360052109\n",
      "\n",
      "Epoch: 28 \tLoss: 1420.6743896007538\n",
      "\n",
      "Epoch: 29 \tLoss: 1420.7459064722061\n",
      "\n",
      "Epoch: 30 \tLoss: 1422.2946004867554\n",
      "\n",
      "Epoch: 31 \tLoss: 1425.3593190908432\n",
      "\n",
      "Epoch: 32 \tLoss: 1429.8400340080261\n",
      "\n",
      "Epoch: 33 \tLoss: 1435.5100923776627\n",
      "\n",
      "Epoch: 34 \tLoss: 1441.691069483757\n",
      "\n",
      "Epoch: 35 \tLoss: 1447.0366567373276\n",
      "\n",
      "Epoch: 36 \tLoss: 1452.3681426048279\n",
      "\n",
      "Epoch: 37 \tLoss: 1459.643948316574\n",
      "\n",
      "Epoch: 38 \tLoss: 1468.340271115303\n",
      "\n",
      "Epoch: 39 \tLoss: 1479.641767501831\n",
      "\n",
      "Epoch: 40 \tLoss: 1492.3665772676468\n",
      "\n",
      "Epoch: 41 \tLoss: 1505.5768758058548\n",
      "\n",
      "Epoch: 42 \tLoss: 1518.8238940238953\n",
      "\n",
      "Epoch: 43 \tLoss: 1531.671777844429\n",
      "\n",
      "Epoch: 44 \tLoss: 1543.0771057605743\n",
      "\n",
      "Epoch: 45 \tLoss: 1553.1212170124054\n",
      "\n",
      "Epoch: 46 \tLoss: 1563.1090849637985\n",
      "\n",
      "Epoch: 47 \tLoss: 1573.9829214811325\n",
      "\n",
      "Epoch: 48 \tLoss: 1584.9516783952713\n",
      "\n",
      "Epoch: 49 \tLoss: 1596.1751391887665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "TZ3_QGKVK6Tj",
    "outputId": "5031b8ec-45df-4fe4-ec90-ee68a49b7284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.011648</td>\n",
       "      <td>-0.047930</td>\n",
       "      <td>0.008394</td>\n",
       "      <td>-0.009676</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>-0.014056</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.041171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014002</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>0.033761</td>\n",
       "      <td>-0.000545</td>\n",
       "      <td>0.033759</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.024722</td>\n",
       "      <td>0.023964</td>\n",
       "      <td>-0.005562</td>\n",
       "      <td>-0.046044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.017240</td>\n",
       "      <td>-0.016410</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.047093</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.040729</td>\n",
       "      <td>0.027776</td>\n",
       "      <td>-0.022116</td>\n",
       "      <td>-0.044522</td>\n",
       "      <td>-0.038054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023983</td>\n",
       "      <td>0.041698</td>\n",
       "      <td>0.021707</td>\n",
       "      <td>0.028402</td>\n",
       "      <td>0.011837</td>\n",
       "      <td>-0.011091</td>\n",
       "      <td>-0.024838</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.046619</td>\n",
       "      <td>0.025107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>-0.020606</td>\n",
       "      <td>0.037060</td>\n",
       "      <td>0.042983</td>\n",
       "      <td>-0.043511</td>\n",
       "      <td>0.022386</td>\n",
       "      <td>-0.045031</td>\n",
       "      <td>-0.040638</td>\n",
       "      <td>-0.047910</td>\n",
       "      <td>0.048303</td>\n",
       "      <td>-0.025121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.041389</td>\n",
       "      <td>0.028991</td>\n",
       "      <td>-0.037136</td>\n",
       "      <td>0.037012</td>\n",
       "      <td>0.037409</td>\n",
       "      <td>0.012226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.033780</td>\n",
       "      <td>0.042973</td>\n",
       "      <td>-0.038912</td>\n",
       "      <td>-0.034784</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>-0.038870</td>\n",
       "      <td>-0.045170</td>\n",
       "      <td>-0.012586</td>\n",
       "      <td>0.031027</td>\n",
       "      <td>-0.042777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039541</td>\n",
       "      <td>-0.013469</td>\n",
       "      <td>0.049048</td>\n",
       "      <td>-0.023739</td>\n",
       "      <td>0.018168</td>\n",
       "      <td>-0.016173</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.035067</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>0.007740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.019298</td>\n",
       "      <td>-0.000263</td>\n",
       "      <td>-0.040538</td>\n",
       "      <td>-0.032503</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>0.025789</td>\n",
       "      <td>-0.027111</td>\n",
       "      <td>-0.035730</td>\n",
       "      <td>0.034403</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032000</td>\n",
       "      <td>0.049171</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>-0.008373</td>\n",
       "      <td>0.037810</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>-0.037655</td>\n",
       "      <td>-0.021353</td>\n",
       "      <td>0.046557</td>\n",
       "      <td>-0.042988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "and -0.011648 -0.047930  0.008394 -0.009676 -0.009229 -0.014056  0.022342   \n",
       "the  0.017240 -0.016410  0.013487  0.047093  0.000067  0.040729  0.027776   \n",
       "my  -0.020606  0.037060  0.042983 -0.043511  0.022386 -0.045031 -0.040638   \n",
       "of   0.033780  0.042973 -0.038912 -0.034784  0.034500 -0.038870 -0.045170   \n",
       "we  -0.019298 -0.000263 -0.040538 -0.032503  0.035841  0.025789 -0.027111   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "and -0.009655  0.003042  0.041171  ... -0.014002  0.019548  0.033761   \n",
       "the -0.022116 -0.044522 -0.038054  ...  0.023983  0.041698  0.021707   \n",
       "my  -0.047910  0.048303 -0.025121  ...  0.012744  0.017239  0.016109   \n",
       "of  -0.012586  0.031027 -0.042777  ...  0.039541 -0.013469  0.049048   \n",
       "we  -0.035730  0.034403 -0.001675  ... -0.032000  0.049171 -0.029834   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "and -0.000545  0.033759  0.023135  0.024722  0.023964 -0.005562 -0.046044  \n",
       "the  0.028402  0.011837 -0.011091 -0.024838  0.002965  0.046619  0.025107  \n",
       "my   0.009494  0.041389  0.028991 -0.037136  0.037012  0.037409  0.012226  \n",
       "of  -0.023739  0.018168 -0.016173  0.000497  0.035067  0.033173  0.007740  \n",
       "we  -0.008373  0.037810 -0.002068 -0.037655 -0.021353  0.046557 -0.042988  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFs2IAn_LAYS",
    "outputId": "9eb48371-f646-40b7-abed-5749b2246f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'america': ['jared', 'joy', 'remarkable', 'amazing', 'before']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['america']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hocGV0BTFlIm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
